{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf7c36e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T17:33:27.289749Z",
     "iopub.status.busy": "2025-12-03T17:33:27.289455Z",
     "iopub.status.idle": "2025-12-03T17:41:30.350154Z",
     "shell.execute_reply": "2025-12-03T17:41:30.349186Z"
    },
    "papermill": {
     "duration": 483.076291,
     "end_time": "2025-12-03T17:41:30.361998",
     "exception": false,
     "start_time": "2025-12-03T17:33:27.285707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimized pipeline on: cpu\n",
      ">>> Loading Target IDs...\n",
      "Total Targets: 224309\n",
      ">>> Loading ESM-2 Data...\n",
      ">>> Preparing Train/Test Matrices...\n",
      "  Valid Train Proteins: 82404\n",
      "  > Creating Labels...\n",
      "\n",
      ">>> TRAINING ESM-2 WITH VALIDATION <<<\n",
      "  Train size: 74163 | Val size: 8241\n",
      "  Epoch 1: Loss 0.0162\n",
      "  Epoch 2: Loss 0.0123\n",
      "  > Validating...\n",
      "  > Calculating F-max...\n",
      "  [RESULT] Best Threshold: 0.22 | Validation F-Max: 0.2989\n",
      "  > Predicting on Test Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 3505/3505 [00:13<00:00, 259.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Formatting Results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 224309/224309 [00:08<00:00, 25816.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Merging with Homology Data...\n",
      "  Final Join...\n",
      ">>> Saving submission.tsv...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def compute_fmax(y_true, y_pred_probs, steps=20):\n",
    "    \"\"\"\n",
    "    Tính F-max bằng cách quét nhiều ngưỡng (threshold) khác nhau.\n",
    "    y_true: Ma trận nhãn thực tế (0 hoặc 1)\n",
    "    y_pred_probs: Ma trận xác suất dự đoán (0.0 đến 1.0)\n",
    "    steps: Số lượng ngưỡng để thử (20 steps nghĩa là thử 0.05, 0.10, ... 0.95)\n",
    "    \"\"\"\n",
    "    print(\"  > Calculating F-max...\")\n",
    "    best_f1 = 0.0\n",
    "    best_threshold = 0.0\n",
    "    \n",
    "    # Chỉ quét các ngưỡng từ 0.05 đến 0.95\n",
    "    thresholds = np.linspace(0.01, 0.99, steps)\n",
    "    \n",
    "    for t in thresholds:\n",
    "        # Chuyển xác suất thành nhãn 0/1 dựa trên ngưỡng t\n",
    "        y_pred_binary = (y_pred_probs >= t).astype(int)\n",
    "        \n",
    "        # Tính F1 (average='micro' hoặc 'samples' thường dùng cho multi-label)\n",
    "        # Trong CAFA chuẩn dùng weighted, ở đây dùng 'samples' để ước lượng nhanh\n",
    "        p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = t\n",
    "            \n",
    "    print(f\"  [RESULT] Best Threshold: {best_threshold:.2f} | Validation F-Max: {best_f1:.4f}\")\n",
    "    return best_f1, best_threshold\n",
    "\n",
    "# ==========================================\n",
    "# 1. CẤU HÌNH \n",
    "# ==========================================\n",
    "class Config:\n",
    "    # Chỉ dùng ESM-2\n",
    "    ESM_DIR = '/kaggle/input/cafa6-protein-embeddings-esm2'\n",
    "    \n",
    "    TRAIN_TERMS = '/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv'\n",
    "    TEST_FASTA = '/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset.fasta'\n",
    "    \n",
    "    # Homology \n",
    "    HOMOLOGY_1 = '/kaggle/input/foldseek-blastp-parthenos/submission.tsv'\n",
    "    HOMOLOGY_2 = '/kaggle/input/foldseek-cafa/foldseek_submission.tsv'\n",
    "    \n",
    "    NUM_LABELS = 1500\n",
    "    BATCH_SIZE = 64      \n",
    "    LR = 0.001\n",
    "    EPOCHS = 2           \n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Running optimized pipeline on: {Config.DEVICE}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. MODEL & DATA UTILS\n",
    "# ==========================================\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_features),\n",
    "            nn.BatchNorm1d(hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_features, in_features),\n",
    "            nn.BatchNorm1d(in_features)\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + self.block(x))\n",
    "\n",
    "class ProteinClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.bn_input = nn.BatchNorm1d(input_dim)\n",
    "        # Giảm kích thước mạng một chút để nhẹ CPU\n",
    "        self.layer1 = nn.Linear(input_dim, 512) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.res1 = ResidualBlock(512, 256)\n",
    "        self.layer_out = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn_input(x)\n",
    "        x = self.dropout(self.relu(self.layer1(x)))\n",
    "        x = self.res1(x)\n",
    "        return self.layer_out(x)\n",
    "\n",
    "def get_label_matrix(train_ids):\n",
    "    print(\"  > Creating Labels...\")\n",
    "    df_terms = pd.read_csv(Config.TRAIN_TERMS, sep=\"\\t\")\n",
    "    top_terms = df_terms['term'].value_counts().index[:Config.NUM_LABELS].tolist()\n",
    "    term_to_idx = {term: i for i, term in enumerate(top_terms)}\n",
    "    \n",
    "    pid_to_idx = {pid: i for i, pid in enumerate(train_ids)}\n",
    "    y_data = np.zeros((len(train_ids), Config.NUM_LABELS), dtype=np.float32)\n",
    "    \n",
    "    # Filter & Fill\n",
    "    df_filtered = df_terms[df_terms['term'].isin(top_terms) & df_terms['EntryID'].isin(train_ids)]\n",
    "    \n",
    "    # Dùng numpy indexing cho nhanh\n",
    "    term_indices = df_filtered['term'].map(term_to_idx).dropna().astype(int).values\n",
    "    pid_indices = df_filtered['EntryID'].map(pid_to_idx).dropna().astype(int).values\n",
    "    \n",
    "    # Đảm bảo độ dài khớp nhau (đôi khi map sinh ra NaN)\n",
    "    valid_len = min(len(term_indices), len(pid_indices))\n",
    "    y_data[pid_indices[:valid_len], term_indices[:valid_len]] = 1.0\n",
    "    \n",
    "    return y_data, term_to_idx\n",
    "\n",
    "# ==========================================\n",
    "# CẬP NHẬT HÀM TRAIN & PREDICT\n",
    "# ==========================================\n",
    "def train_and_predict_with_score(X, y, X_test, model_name):\n",
    "    print(f\"\\n>>> TRAINING {model_name} WITH VALIDATION <<<\")\n",
    "    \n",
    "    # 1. Chia tập Train thành Train (90%) và Val (10%) để tính điểm\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    \n",
    "    print(f\"  Train size: {len(X_train)} | Val size: {len(X_val)}\")\n",
    "    \n",
    "    # Dataset\n",
    "    train_ds = torch.utils.data.TensorDataset(torch.tensor(X_train).float(), torch.tensor(y_train).float())\n",
    "    # Không cần shuffle cho Val và Test\n",
    "    val_ds = torch.utils.data.TensorDataset(torch.tensor(X_val).float()) \n",
    "    test_ds = torch.utils.data.TensorDataset(torch.tensor(X_test).float())\n",
    "    \n",
    "    # Dataloader (CPU optimized)\n",
    "    train_dl = DataLoader(train_ds, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_dl = DataLoader(val_ds, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    test_dl = DataLoader(test_ds, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Model Setup\n",
    "    model = ProteinClassifier(X_train.shape[1], Config.NUM_LABELS).to(Config.DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=Config.LR)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # --- TRAINING LOOP ---\n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, targets in train_dl:\n",
    "            inputs, targets = inputs.to(Config.DEVICE), targets.to(Config.DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        print(f\"  Epoch {epoch+1}: Loss {train_loss/len(train_dl):.4f}\")\n",
    "        \n",
    "    # --- VALIDATION & SCORING ---\n",
    "    print(\"  > Validating...\")\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    with torch.no_grad():\n",
    "        for (inputs,) in val_dl:\n",
    "            inputs = inputs.to(Config.DEVICE)\n",
    "            logits = model(inputs)\n",
    "            val_preds.append(torch.sigmoid(logits).cpu().numpy())\n",
    "    \n",
    "    val_preds = np.vstack(val_preds)\n",
    "    \n",
    "    # Gọi hàm tính F-max tại đây\n",
    "    fmax, best_thresh = compute_fmax(y_val, val_preds)\n",
    "    \n",
    "    # --- PREDICTION ON TEST SET ---\n",
    "    print(\"  > Predicting on Test Set...\")\n",
    "    test_preds = []\n",
    "    with torch.no_grad():\n",
    "        for (inputs,) in tqdm(test_dl, desc=\"Inference\"):\n",
    "            inputs = inputs.to(Config.DEVICE)\n",
    "            logits = model(inputs)\n",
    "            test_preds.append(torch.sigmoid(logits).cpu().numpy())\n",
    "            \n",
    "    return np.vstack(test_preds)\n",
    "\n",
    "# ==========================================\n",
    "# 3. PIPELINE CHÍNH (ESM-2 ONLY)\n",
    "# ==========================================\n",
    "\n",
    "# A. Lấy danh sách ID cần dự đoán\n",
    "print(\">>> Loading Target IDs...\")\n",
    "test_ids_list = []\n",
    "with open(Config.TEST_FASTA, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('>'):\n",
    "            test_ids_list.append(line.strip()[1:].split()[0])\n",
    "print(f\"Total Targets: {len(test_ids_list)}\")\n",
    "\n",
    "# B. Load ESM-2 Data\n",
    "print(\">>> Loading ESM-2 Data...\")\n",
    "# Load IDs an toàn\n",
    "try:\n",
    "    df_ids = pd.read_csv(os.path.join(Config.ESM_DIR, \"protein_ids.csv\"))\n",
    "    all_pids = df_ids[\"protein_id\"].tolist()\n",
    "except:\n",
    "    all_pids_raw = np.load(os.path.join(Config.ESM_DIR, \"protein_ids.npy\"), allow_pickle=True)\n",
    "    all_pids = [i.decode('utf-8') if isinstance(i, (bytes, np.bytes_)) else i for i in all_pids_raw]\n",
    "\n",
    "# Load Embeddings (mmap)\n",
    "all_embeds = np.load(os.path.join(Config.ESM_DIR, \"protein_embeddings.npy\"), mmap_mode='r')\n",
    "pid_to_idx_map = {pid: i for i, pid in enumerate(all_pids)}\n",
    "\n",
    "# C. Chuẩn bị Train/Test Arrays\n",
    "print(\">>> Preparing Train/Test Matrices...\")\n",
    "# Xác định Train IDs\n",
    "df_terms = pd.read_csv(Config.TRAIN_TERMS, sep=\"\\t\")\n",
    "train_pids_target = set(df_terms['EntryID'].unique())\n",
    "valid_train = [pid for pid in train_pids_target if pid in pid_to_idx_map]\n",
    "\n",
    "print(f\"  Valid Train Proteins: {len(valid_train)}\")\n",
    "\n",
    "# X_train\n",
    "train_indices = [pid_to_idx_map[pid] for pid in valid_train]\n",
    "X_train = np.array([all_embeds[i] for i in train_indices])\n",
    "y_train, term_map = get_label_matrix(valid_train)\n",
    "\n",
    "# X_test\n",
    "emb_dim = X_train.shape[1]\n",
    "X_test = np.zeros((len(test_ids_list), emb_dim), dtype=np.float32)\n",
    "for i, pid in enumerate(test_ids_list):\n",
    "    if pid in pid_to_idx_map:\n",
    "        X_test[i] = all_embeds[pid_to_idx_map[pid]]\n",
    "\n",
    "# D. Train & Predict\n",
    "preds_dl = train_and_predict_with_score(X_train, y_train, X_test, \"ESM-2\")\n",
    "\n",
    "# Dọn dẹp RAM ngay\n",
    "del X_train, y_train, all_embeds, df_terms, train_indices\n",
    "gc.collect()\n",
    "\n",
    "# E. Format DL Results\n",
    "print(\">>> Formatting Results...\")\n",
    "idx_to_term = {i: t for t, i in term_map.items()}\n",
    "dl_results = []\n",
    "\n",
    "for i, pid in enumerate(tqdm(test_ids_list)):\n",
    "    scores = preds_dl[i]\n",
    "    # Lấy top scores > 0.01\n",
    "    indices = np.where(scores > 0.01)[0]\n",
    "    for idx in indices:\n",
    "        dl_results.append((pid, idx_to_term[idx], scores[idx]))\n",
    "\n",
    "df_dl = pd.DataFrame(dl_results, columns=['Id', 'Term', 'Score'])\n",
    "del preds_dl\n",
    "gc.collect()\n",
    "\n",
    "# ==========================================\n",
    "# 4. HOMOLOGY MERGE & SAVE\n",
    "# ==========================================\n",
    "print(\">>> Merging with Homology Data...\")\n",
    "\n",
    "try:\n",
    "    # Load Homology\n",
    "    df_hom1 = pd.read_csv(Config.HOMOLOGY_1, sep='\\t', header=None, names=['Id', 'Term', 'Score_1'])\n",
    "    df_hom2 = pd.read_csv(Config.HOMOLOGY_2, sep='\\t', header=None, names=['Id', 'Term', 'Score_2'])\n",
    "    \n",
    "    # Merge Homology (Max)\n",
    "    df_hom_merged = pd.merge(df_hom1, df_hom2, on=['Id', 'Term'], how='outer').fillna(0)\n",
    "    df_hom_merged['Score_Homology'] = np.maximum(df_hom_merged['Score_1'], df_hom_merged['Score_2'])\n",
    "    df_hom_merged = df_hom_merged[['Id', 'Term', 'Score_Homology']]\n",
    "    \n",
    "    # Merge with DL\n",
    "    print(\"  Final Join...\")\n",
    "    df_final = pd.merge(df_dl, df_hom_merged, on=['Id', 'Term'], how='outer').fillna(0)\n",
    "    \n",
    "    # Logic: Max(DL, Homology)\n",
    "    df_final['Final_Score'] = np.maximum(df_final['Score'], df_final['Score_Homology'])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Warning: Homology merge failed ({e}). Saving DL only.\")\n",
    "    df_final = df_dl\n",
    "    df_final['Final_Score'] = df_final['Score']\n",
    "\n",
    "# Save\n",
    "print(\">>> Saving submission.tsv...\")\n",
    "submission = df_final[['Id', 'Term', 'Final_Score']]\n",
    "submission.columns = ['Id', 'Term', 'Score']\n",
    "submission = submission[submission['Score'] > 0.01]\n",
    "submission['Score'] = submission['Score'].round(3)\n",
    "\n",
    "submission.to_csv('submission.tsv', sep='\\t', header=False, index=False)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14084779,
     "isSourceIdPinned": false,
     "sourceId": 116062,
     "sourceType": "competition"
    },
    {
     "datasetId": 3458902,
     "sourceId": 6046570,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8559888,
     "sourceId": 13482672,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8618696,
     "sourceId": 13567659,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8688768,
     "sourceId": 13665986,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 490.313514,
   "end_time": "2025-12-03T17:41:33.193279",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-03T17:33:22.879765",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
